{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    "\n",
    "In this exercise, we will familiarize ourselves with text-generations model via `transformers`.\n",
    "\n",
    "### Exercise 1(a) (2 points)\n",
    "\n",
    "Load `pipeline` from `transformers`. Then, load the `gpt2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text-generator pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1(b) (3 points)\n",
    "\n",
    "Using the pre-trained `gpt2` model, write a two sentence and generate text after those two sentences with the `gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"this is my first time using the gpt2 model. I'm really excited to see what it can do.\\n\\n\\nA big thanks to everyone who sent feedback and suggestions. I really did not expect the gpt2 models so soon. I\"},\n",
       " {'generated_text': \"this is my first time using the gpt2 model. I'm really excited to see what it can do.\\n\\nMore interesting is this:\\n\\nWhat's happening with a 2.4GHz CPU? It's not all going according to\"},\n",
       " {'generated_text': \"this is my first time using the gpt2 model. I'm really excited to see what it can do.\\n\\nAfter working with the gpt2 for 15 months I finally decided to put the sensor on my iPhone which is a 4.\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this is my first time using the gpt2 model. I'm really excited to see what it can do.\"\n",
    "\n",
    "generator(text, max_length=50, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 \n",
    "\n",
    "In this exercise, we will familiarize ourselves with named-entity-recognition models via `tranformers`.\n",
    "\n",
    "### Exercise 2(a) (2 points)\n",
    "\n",
    "Using the `pipeline`, load `dslim/bert-base-NER` with `task=ner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gmgma\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(task=\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(b) (3 points)\n",
    "\n",
    "Use the pre-trained model from part (a) to identify entities in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Oscar, Score: 0.9996612071990967, Type: B-PER\n",
      "Entity: A, Score: 0.9996281862258911, Type: I-PER\n",
      "Entity: ##gu, Score: 0.999373733997345, Type: I-PER\n",
      "Entity: ##ila, Score: 0.9955099821090698, Type: I-PER\n",
      "Entity: ##r, Score: 0.9710258841514587, Type: I-PER\n",
      "Entity: Grand, Score: 0.9955690503120422, Type: B-ORG\n",
      "Entity: View, Score: 0.9942438006401062, Type: I-ORG\n",
      "Entity: University, Score: 0.9948624968528748, Type: I-ORG\n",
      "Entity: Iowa, Score: 0.9984671473503113, Type: B-LOC\n"
     ]
    }
   ],
   "source": [
    "text = \"Oscar Aguilar is an awesome data professor at Grand View University, a small college in Iowa.\"\n",
    "\n",
    "for entity in ner(text):\n",
    "    print(f\"Entity: {entity['word']}, Score: {entity['score']}, Type: {entity['entity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 \n",
    "\n",
    "In this exercise, we will familiarize ourselves with summarization models via `transformers`.\n",
    "\n",
    "### Exercise 3(a) (2 points)\n",
    "\n",
    "Using the `pipeline`, load `facebook/bart-large-cnn` with `task=summarization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gmgma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gmgma\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "# loading the summarizer\n",
    "summarizer = pipeline(task=\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3(b) (3 points)\n",
    "\n",
    "Given the below text:\n",
    "\n",
    "```\n",
    "The deep learning field has been experiencing a seismic shift, thanks to the emergence and rapid evolution of Transformer models.\n",
    "These groundbreaking architectures have not just redefined the standards in Natural Language Processing (NLP) but have broadened their horizons to revolutionize numerous facets of artificial intelligence.\n",
    "Characterized by their unique attention mechanisms and parallel processing abilities, Transformer models stand as a testament to the innovative leaps in understanding and generating human language with an accuracy and efficiency previously unattainable.\n",
    "First appeared in 2017 in the “Attention is all you need” article by Google, the transformer architecture is at the heart of groundbreaking models like ChatGPT, sparking a new wave of excitement in the AI community. They've been instrumental in OpenAI's cutting-edge language models and played a key role in DeepMind's AlphaStar.\n",
    "In this transformative era of AI, the significance of Transformer models for aspiring data scientists and NLP practitioners cannot be overstated.\n",
    "As one of the core fields for most of the latest technological leap forwards, this article aims to decipher the secrets behind these models.\n",
    "```\n",
    "\n",
    "Summarize the above text in no more than 50 words and at least 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"The transformer architecture is at the heart of groundbreaking models like ChatGPT, sparking a new wave of excitement in the AI community. They've been instrumental in OpenAI's cutting-edge language models and played a key role in DeepMind\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "The deep learning field has been experiencing a seismic shift, thanks to the emergence and rapid evolution of Transformer models.\n",
    "These groundbreaking architectures have not just redefined the standards in Natural Language Processing (NLP) but have broadened their horizons to revolutionize numerous facets of artificial intelligence.\n",
    "Characterized by their unique attention mechanisms and parallel processing abilities, Transformer models stand as a testament to the innovative leaps in understanding and generating human language with an accuracy and efficiency previously unattainable.\n",
    "First appeared in 2017 in the “Attention is all you need” article by Google, the transformer architecture is at the heart of groundbreaking models like ChatGPT, sparking a new wave of excitement in the AI community. They've been instrumental in OpenAI's cutting-edge language models and played a key role in DeepMind's AlphaStar.\n",
    "In this transformative era of AI, the significance of Transformer models for aspiring data scientists and NLP practitioners cannot be overstated.\n",
    "As one of the core fields for most of the latest technological leap forwards, this article aims to decipher the secrets behind these models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summarizer(text, max_length=50, min_length=25, do_sample=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
